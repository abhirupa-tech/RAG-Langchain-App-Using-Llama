{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Read Content From PDf - To be Used For RAG</h2>\n",
    "Post Reading, store in a file as Datafram, woith each Paragraph/ Page content as a Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\GitHub\\RAG-Langchain-App-Using-Llama\\llama3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "# 2. load model\n",
    "SentenceTransformer = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\GitHub\\RAG-Langchain-App-Using-Llama\\llama3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          TrainingData  \\\n",
      "0    Chapter 1  - Our Picture of the UniverseChapte...   \n",
      "1    future (or have we?) but I discuss a possible ...   \n",
      "2    CHAPTER 1OUR PICTURE OF THE UNIVERSE A well-kn...   \n",
      "3    Figure 1:1The planets themselves moved on smal...   \n",
      "4    Kepler, and the Italian, Galileo Galilei – sta...   \n",
      "..                                                 ...   \n",
      "96   GLOSSARY Absolute zero : The lowest possible t...   \n",
      "97   Elementary particle : A particle that, it is b...   \n",
      "98   force.Particle accelerator : A machine that, u...   \n",
      "99   Stationary state : One that is not changing wi...   \n",
      "100  ACKNOWLEDGMENTS Many people have helped me in ...   \n",
      "\n",
      "                                            Embeddings  \n",
      "0    [0.120179795, 0.15168652, 0.41615, -0.82843953...  \n",
      "1    [0.029344482, 0.0025265208, 0.46344772, -0.714...  \n",
      "2    [0.20948802, 0.2716869, 0.5334739, -0.9225296,...  \n",
      "3    [0.18583146, 0.3157446, 0.78558713, -1.42911, ...  \n",
      "4    [0.103784785, 0.0420905, 0.5655542, -0.9685477...  \n",
      "..                                                 ...  \n",
      "96   [-0.10375858, 0.08440834, 0.58591527, -0.18975...  \n",
      "97   [-0.4156252, -0.45168108, 0.12438713, -0.22751...  \n",
      "98   [-0.010929952, -0.37008622, 0.38307413, -0.285...  \n",
      "99   [-0.14516786, -0.1962474, 0.54584897, -0.15149...  \n",
      "100  [0.38747317, -0.010221307, 0.11098081, -0.4937...  \n",
      "\n",
      "[101 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "reader = PdfReader(\"sourceData\\A Brief History of Time .. Stephen Hawking.pdf\")\n",
    "number_of_pages = len(reader.pages)\n",
    "textChunksDF =  pd.DataFrame(columns=['TrainingData', 'Embeddings'])\n",
    "\n",
    "#Read content from all pages, compile them into One Txt File\n",
    "for page in range(number_of_pages):\n",
    "    # pageText = reader.pages[page]\n",
    "    plainText = reader.pages[page].extract_text()\n",
    "    newRow = {'TrainingData':plainText.replace(\"\\n\", \"\"),'Embeddings': SentenceTransformer.encode(plainText)}\n",
    "    textChunksDF = textChunksDF._append(newRow, ignore_index=True)\n",
    "\n",
    "print(textChunksDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65291b288b1a4b0591b05c29638d7ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "# import faiss.cpu as faiss\n",
    "\n",
    "data = Dataset.from_pandas(textChunksDF)\n",
    "data = data.add_faiss_index(\"Embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
